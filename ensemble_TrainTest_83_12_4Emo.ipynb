{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "PQDMHzQxW4D6",
        "outputId": "2afdc2d8-60ff-46b7-807f-5dbb1178b6a0"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Import All Important Libraries\n",
        "import librosa\n",
        "import soundfile\n",
        "import os, glob, pickle\n",
        "import numpy as np\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.ensemble import RandomForestClassifier, VotingClassifier\n",
        "from sklearn.svm import SVC\n",
        "from sklearn.neighbors import KNeighborsClassifier\n",
        "from sklearn.metrics import accuracy_score\n",
        "from sklearn.pipeline import Pipeline\n",
        "from sklearn.preprocessing import StandardScaler \n",
        "from sklearn.feature_selection import RFE\n",
        "from sklearn.neural_network import MLPClassifier"
      ],
      "metadata": {
        "id": "q6pmo4UwW8W2"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# function for extracting mfcc, pitch, and rmse features from sound file\n",
        "def extract_feature(file_name, mfcc, pitch, rmse):\n",
        "    with soundfile.SoundFile(file_name) as sound_file:\n",
        "        X = sound_file.read(dtype=\"float32\")\n",
        "        sample_rate = sound_file.samplerate\n",
        "        result = []\n",
        "        if mfcc:\n",
        "            mfccs = librosa.feature.mfcc(y=X, sr=sample_rate, n_mfcc=40)\n",
        "            if mfccs.size > 0:\n",
        "                mfccs = np.mean(mfccs.T, axis=0)\n",
        "                mfccs = np.pad(mfccs, (0, 40 - len(mfccs)), mode='constant')\n",
        "                result.append(mfccs)\n",
        "        if pitch:\n",
        "            f0, voiced_flag, voiced_probs = librosa.pyin(y=X, sr=sample_rate, fmin=librosa.note_to_hz('C2'), fmax=librosa.note_to_hz('C7'))\n",
        "            if np.any(voiced_flag):\n",
        "                pitch_mean = np.mean(f0[voiced_flag])\n",
        "                pitch_std = np.std(f0[voiced_flag])\n",
        "                if np.isnan(pitch_mean) or np.isnan(pitch_std):\n",
        "                    return None\n",
        "                pitch = np.array([pitch_mean, pitch_std])\n",
        "                pitch = np.pad(pitch, (0, 2 - len(pitch)), mode='constant')\n",
        "                result.append(pitch)\n",
        "       \n",
        "        if rmse:\n",
        "            rmse = librosa.feature.rms(y=X)\n",
        "            if rmse.size > 0:\n",
        "                rmse = np.mean(rmse.T, axis=0)\n",
        "                rmse = np.pad(rmse, (0, 1 - len(rmse)), mode='constant')\n",
        "                result.append(rmse)\n",
        "        \n",
        "    return np.concatenate(result)\n"
      ],
      "metadata": {
        "id": "4loxI5ztXATl"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Define the motions dictionary\n",
        "emotions = {\n",
        "    '01': 'neutral',\n",
        "    '02': 'calm',\n",
        "    '03': 'happy',\n",
        "    '04': 'sad',\n",
        "    '05': 'angry',\n",
        "    '06': 'fearful',\n",
        "    '07': 'disgust',\n",
        "    '08': 'surprised'\n",
        "}\n",
        "\n",
        "# Emotions we want to observe\n",
        "observed_emotions = ['sad', 'angry', 'fearful', 'surprised']\n",
        "\n",
        "# Load the data and extract features for each sound file\n",
        "def load_data(test_size=0.1):\n",
        "    x, y = [], []\n",
        "#      for folder in glob.glob('C:\\\\Users\\\\user\\\\Documents\\\\6th_sem_project\\\\speech-emotion-recognition-ravdess-data\\\\Actor_' + '*'):\n",
        "#      for folder in glob.glob('C:\\\\Users\\\\user\\\\Documents\\\\6th_sem_project\\\\Reduced dataset\\\\Actor_' + '*'):\n",
        "#      for folder in glob.glob('/content/drive/MyDrive/speech/Actor_' + '*'):\n",
        "    for folder in glob.glob('/content/drive/MyDrive/Dataset_Speech_Emotion_Recognition/speech-emotion-recognition-ravdess-data/Actor_' + '*'):\n",
        "        print(folder)\n",
        "        for file in glob.glob(folder + '/*.wav'):\n",
        "            file_name = os.path.basename(file)\n",
        "            emotion = emotions[file_name.split('-')[2]]\n",
        "            if emotion not in observed_emotions:\n",
        "                continue\n",
        "            feature = extract_feature(file, mfcc=True, pitch=True, rmse=True)\n",
        "            if feature is not None:\n",
        "                x.append(feature)\n",
        "                y.append(emotion)\n",
        "    \n",
        "    max_len = max(len(l) for l in x)\n",
        "    x = [np.pad(l, pad_width=(0, max_len - len(l)), mode='constant') for l in x]\n",
        "    \n",
        "    # convert x to a 2D array of numeric values\n",
        "    x = np.vstack(x)\n",
        "    \n",
        "    return train_test_split(np.array(x, dtype=object), np.array(y, dtype=object), test_size=test_size, random_state=9)\n",
        "\n",
        "\n",
        "# Load the data and split into train and test sets\n",
        "x_train, x_test, y_train, y_test = load_data(test_size=0.1)\n",
        "\n",
        "# Print shape of train and test set and number of features extracted\n",
        "print((x_train.shape[0], x_test.shape[0]))\n",
        "print(f'Features extracted: {x_train[0].shape[0]}')\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "b2oc67vwXSiT",
        "outputId": "6815f379-79cf-443b-8971-cf6fc614d1b5"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/content/drive/MyDrive/Dataset_Speech_Emotion_Recognition/speech-emotion-recognition-ravdess-data/Actor_18\n",
            "/content/drive/MyDrive/Dataset_Speech_Emotion_Recognition/speech-emotion-recognition-ravdess-data/Actor_19\n",
            "/content/drive/MyDrive/Dataset_Speech_Emotion_Recognition/speech-emotion-recognition-ravdess-data/Actor_16\n",
            "/content/drive/MyDrive/Dataset_Speech_Emotion_Recognition/speech-emotion-recognition-ravdess-data/Actor_23\n",
            "/content/drive/MyDrive/Dataset_Speech_Emotion_Recognition/speech-emotion-recognition-ravdess-data/Actor_21\n",
            "/content/drive/MyDrive/Dataset_Speech_Emotion_Recognition/speech-emotion-recognition-ravdess-data/Actor_20\n",
            "/content/drive/MyDrive/Dataset_Speech_Emotion_Recognition/speech-emotion-recognition-ravdess-data/Actor_17\n",
            "/content/drive/MyDrive/Dataset_Speech_Emotion_Recognition/speech-emotion-recognition-ravdess-data/Actor_22\n",
            "/content/drive/MyDrive/Dataset_Speech_Emotion_Recognition/speech-emotion-recognition-ravdess-data/Actor_24\n",
            "/content/drive/MyDrive/Dataset_Speech_Emotion_Recognition/speech-emotion-recognition-ravdess-data/Actor_09\n",
            "/content/drive/MyDrive/Dataset_Speech_Emotion_Recognition/speech-emotion-recognition-ravdess-data/Actor_06\n",
            "/content/drive/MyDrive/Dataset_Speech_Emotion_Recognition/speech-emotion-recognition-ravdess-data/Actor_12\n",
            "/content/drive/MyDrive/Dataset_Speech_Emotion_Recognition/speech-emotion-recognition-ravdess-data/Actor_14\n",
            "/content/drive/MyDrive/Dataset_Speech_Emotion_Recognition/speech-emotion-recognition-ravdess-data/Actor_11\n",
            "/content/drive/MyDrive/Dataset_Speech_Emotion_Recognition/speech-emotion-recognition-ravdess-data/Actor_13\n",
            "/content/drive/MyDrive/Dataset_Speech_Emotion_Recognition/speech-emotion-recognition-ravdess-data/Actor_07\n",
            "/content/drive/MyDrive/Dataset_Speech_Emotion_Recognition/speech-emotion-recognition-ravdess-data/Actor_10\n",
            "/content/drive/MyDrive/Dataset_Speech_Emotion_Recognition/speech-emotion-recognition-ravdess-data/Actor_05\n",
            "/content/drive/MyDrive/Dataset_Speech_Emotion_Recognition/speech-emotion-recognition-ravdess-data/Actor_08\n",
            "/content/drive/MyDrive/Dataset_Speech_Emotion_Recognition/speech-emotion-recognition-ravdess-data/Actor_04\n",
            "/content/drive/MyDrive/Dataset_Speech_Emotion_Recognition/speech-emotion-recognition-ravdess-data/Actor_01\n",
            "/content/drive/MyDrive/Dataset_Speech_Emotion_Recognition/speech-emotion-recognition-ravdess-data/Actor_02\n",
            "/content/drive/MyDrive/Dataset_Speech_Emotion_Recognition/speech-emotion-recognition-ravdess-data/Actor_03\n",
            "/content/drive/MyDrive/Dataset_Speech_Emotion_Recognition/speech-emotion-recognition-ravdess-data/Actor_15\n",
            "(691, 77)\n",
            "Features extracted: 43\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# create the pipeline\n",
        "pipeline = Pipeline([\n",
        "    ('scaler', StandardScaler()),\n",
        "    ('feature_selection', RFE(estimator=RandomForestClassifier(), n_features_to_select=37)),\n",
        "     ('classifier', VotingClassifier( estimators=[\n",
        "        ('rf', RandomForestClassifier(n_estimators=500, random_state=9)),\n",
        "        ('svc', SVC(kernel='linear', probability=True, random_state=9)),\n",
        "        ('knn', KNeighborsClassifier(n_neighbors=6)),\n",
        "        ('mlp', MLPClassifier(alpha=0.01, batch_size=256, hidden_layer_sizes=(700,), max_iter=500, learning_rate='adaptive', random_state=9))\n",
        "    ], voting='soft'))\n",
        "])\n",
        "\n",
        "\n",
        "# Fit the model to the training data\n",
        "pipeline.fit(x_train, y_train)\n",
        "\n",
        "# Predict for the test set\n",
        "y_pred = pipeline.predict(x_test)\n",
        "\n",
        "# Calculate accuracy\n",
        "accuracy = accuracy_score(y_test, y_pred) \n",
        "print(f\"Accuracy score: {accuracy}\")\n",
        "print(\"Accuracy: {:.2f}%\".format(accuracy*100))"
      ],
      "metadata": {
        "id": "4iwR3drZW8aV",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "95d7bab8-06ca-4624-980e-550bcd8864a0"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Accuracy score: 0.8311688311688312\n",
            "Accuracy: 83.12%\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "filename = \"ensemble_model\"\n",
        "pickle.dump(pipeline, open(filename, 'wb'))"
      ],
      "metadata": {
        "id": "Adhk-2MlP1i2"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        " path = '/content/drive/MyDrive/Dataset_Speech_Emotion_Recognition/ensemble_model'\n",
        "loaded_model = pickle.load(open(path, 'rb'))\n",
        "test = loaded_model.predict(x_test)"
      ],
      "metadata": {
        "id": "0p9cv7mcEPYw"
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Calculate accuracy\n",
        "accuracy = accuracy_score(y_test, test) \n",
        "print(f\"Accuracy score: {accuracy}\")\n",
        "print(\"Accuracy: {:.2f}%\".format(accuracy*100))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "DLL1sPAFGBs4",
        "outputId": "961111f5-d254-4e60-8726-a2dd0ef669a9"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Accuracy score: 0.8311688311688312\n",
            "Accuracy: 83.12%\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "NcQKyupaGRsq"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}